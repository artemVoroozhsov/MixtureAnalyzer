# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F45-Th1RsfdSAE8EwGIsYtu3wm1h6-lO
"""

!pip install unrar
!pip install --quiet optuna
import numpy as np
import random
import torch
import matplotlib.pyplot as plt


def set_random_seed(seed):
    torch.backends.cudnn.deterministic = True
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)


set_random_seed(42)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

!unrar x /content/dataset_red_2.rar
x_train = np.load('/content/x_train_red.npy')
y_train = np.load('/content/y_train_red.npy')
x_val = np.load('/content/x_val_red.npy')
y_val = np.load('/content/y_val_red.npy')

from torch.utils.data import Dataset

class SiameseDataset(Dataset):

  def __init__(self, x, y, transform = None):
    self.x = x
    self.transform = transform
    self.y = y.reshape(-1)

  def __getitem__(self, idx):
    anchor = self.x[idx]
    same_class_idx = np.arange(self.x.shape[0])[idx//20 * 20 : (idx//20 + 1) * 20]
    other_class_idx = np.concatenate((np.arange(self.x.shape[0])[0 : idx//20 * 20],
                                      np.arange(self.x.shape[0])[(idx//20 + 1) * 20 :]))


    positive = self.x[np.random.choice(same_class_idx)]
    positive = positive + self.x[np.random.choice(other_class_idx)] + self.x[np.random.choice(other_class_idx)]

    negative = self.x[np.random.choice(other_class_idx)]
    negative = negative + self.x[np.random.choice(other_class_idx)] + self.x[np.random.choice(other_class_idx)]
    if self.transform is not None:
      anchor = self.transform(anchor)
      positive = self.transform(positive)
      negative = self.transform(negative)
    return anchor, positive, negative


  def __len__(self):
    return len(self.x)

class ValidationSiameseDataset(Dataset):

  def __init__(self, x, y, transform):
    self.x = x
    self.transform = transform
    self.y = y.reshape(-1)

  def __getitem__(self, idx):
    anchor = self.x[idx]
    same_class_idx = np.arange(self.x.shape[0])[idx//5 * 5 : (idx//5 + 1) * 5]
    other_class_idx = np.concatenate((np.arange(self.x.shape[0])[0 : idx//5 * 5],
                                      np.arange(self.x.shape[0])[(idx//5 + 1) * 5 :]))

    positive = self.x[np.random.choice(same_class_idx)]
    positive = positive + self.x[np.random.choice(other_class_idx)] + self.x[np.random.choice(other_class_idx)]

    negative = self.x[np.random.choice(other_class_idx)]
    negative = negative + self.x[np.random.choice(other_class_idx)] + self.x[np.random.choice(other_class_idx)]

    if self.transform is not None:
      anchor = self.transform(anchor)
      positive = self.transform(positive)
      negative = self.transform(negative)
    return anchor, positive, negative



  def __len__(self):
    return len(self.x)

def transformation(x): #add noise to spectra
  x = x/np.max(x)
  x = x + np.random.normal(0,0.005,x.shape[0])
  x = torch.Tensor(x)
  return x

from torch.utils.data import  DataLoader

train_dataset = SiameseDataset(x_train[2000:], y_train[2000:], transform = transformation)
val_dataset = ValidationSiameseDataset(x_val[500:], y_val[500:], transform=transformation)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)

from torch import nn
class SiameseNet(nn.Module):

    def __init__(self, latent_dim):
      super().__init__()
      self.latent_dim = latent_dim
      self.model = nn.Sequential(nn.Conv1d(in_channels=1, out_channels=16,kernel_size=21, padding = 'same'),
                                  nn.MaxPool1d(kernel_size=2, stride=2),
                                  nn.LeakyReLU(),
                                  nn.BatchNorm1d(16),
                                  nn.Conv1d(16, 32, 11, padding = 'same'),
                                  nn.MaxPool1d(2, 2),
                                  nn.LeakyReLU(),
                                  nn.BatchNorm1d(32),
                                  nn.Conv1d(32, 64, 5, padding = 'same'),
                                  nn.MaxPool1d(2, 2),
                                  nn.LeakyReLU(),
                                  nn.BatchNorm1d(64),

                                 nn.Flatten(),
                                 nn.Linear(64*625, self.latent_dim))


    def _forward(self, x):
      out = x.view(-1, 1, 5000)
      out = self.model(out)
      # normalize embedding to unit vector
      out = torch.nn.functional.normalize(out)
      return out


    def predict(self, x):
      out = x.view(-1, 1, 5000)
      out = self.model(out)
      out = torch.nn.functional.normalize(out)
      return out

    def forward(self, anchor, positive, negative, latent_dim):
        output1 = self._forward(anchor)
        output2 = self._forward(positive)
        output3 = self._forward(negative)

        return output1, output2, output3

def train(num_epochs, model, criterion_train, criterion_val, optimizer, train_loader, val_loader, latent_dim):
    loss_history = []
    l = []
    l_val = []
    n_p_train = []
    n_p_val = []

    for epoch in range(0, num_epochs):
      model.train()
      for i, batch in enumerate(train_loader, 0):
          anc, pos, neg = batch
          output_anc, output_pos, output_neg = model(anc.to(device), pos.to(device), neg.to(device), latent_dim)
          loss, number_pos = criterion_train(output_anc, output_pos, output_neg)
          loss.backward()
          optimizer.step()
          optimizer.zero_grad()
          l.append(loss.item())
          n_p_train.append(number_pos.item())

      model.eval()
      for i, batch in enumerate(val_loader, 0):
          anc, pos, neg = batch
          output_anc, output_pos, output_neg = model(anc.to(device), pos.to(device), neg.to(device), latent_dim)
          loss_val, number_pos_val = criterion_val(output_anc, output_pos, output_neg)
          l_val.append(loss_val.item())
          n_p_val.append(number_pos_val.item())
      scheduler.step()
      last_epoch_loss =  torch.tensor(l[-len(train_loader):-1]).mean()
      last_epoch_val_loss = torch.tensor(l_val[-len(val_loader):-1]).mean()
      print("Epoch {} with {:.4f} loss and {:.4f} val_loss".format(epoch, last_epoch_loss, last_epoch_val_loss))

    return l, last_epoch_loss, l_val, last_epoch_val_loss, n_p_train, n_p_val

import torch.nn.functional as F
class TripletLoss(nn.Module):
    """
    Triplet loss
    Takes embeddings of an anchor sample, a positive sample and a negative sample
    """

    def __init__(self, margin, semi_hard):
        super(TripletLoss, self).__init__()
        self.margin = margin
        self.semi_hard = semi_hard

    def forward(self, anchor, positive, negative, size_average=True):
        distance_positive = 1.0 - F.cosine_similarity(anchor, positive)
        distance_negative = 1.0 - F.cosine_similarity(anchor, negative)
        losses = F.relu(distance_positive - distance_negative + self.margin)
        losses = torch.where(losses > self.semi_hard, losses, torch.zeros(losses.shape).to(device))
        return losses.sum()/torch.count_nonzero(losses) if size_average else losses.sum(), torch.count_nonzero(losses)

import optuna
from optuna.samplers import RandomSampler
import torch.optim as optim

# define function which will optimized
def objective(trial):
    # boundaries for the optimizer's
    lr = trial.suggest_float("lr", 1e-5, 1e-2)
    latent_dim = trial.suggest_int("latent_dim", 8, 64, step=8)
    semi_hard = 0.5

    # create new model(and all parameters) every iteration
    model = SiameseNet(latent_dim).to(device)
    criterion_train = TripletLoss(margin = 1, semi_hard = semi_hard)
    criterion_val = TripletLoss(margin = 1, semi_hard = semi_hard)
    optimizer = optim.AdamW(
        model.parameters(), lr=lr
    )  # learning step regulates by optuna

    # To save time, we will take only 4 epochs
    train_loader = DataLoader(train_dataset, num_workers=2, batch_size=128, shuffle=True)
    val_loader = DataLoader(val_dataset,  num_workers=2, batch_size=128, shuffle=False)
    l, _, l_val, __, n_p_train, n_p_val = train(4, model, criterion_train, criterion_val, optimizer, train_loader, val_loader, latent_dim)
    return np.array(n_p_val).reshape(4, 16)[-1].mean()

# Create "exploration"
study = optuna.create_study(direction="minimize", study_name="Optimizer", sampler=RandomSampler(42))

study.optimize(
    objective, n_trials=20
)  # The more iterations, the higher the chances of catching the most optimal hyperparameters